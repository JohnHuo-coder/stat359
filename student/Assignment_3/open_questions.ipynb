{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7a35b",
   "metadata": {},
   "source": [
    "1. Training Dynamics\n",
    "\n",
    "- Did your models show signs of overfitting or underfitting? What architectural or training changes could address this?\n",
    "- How did using class weights affect training stability and final performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0d357",
   "metadata": {},
   "source": [
    "Yes, both the mlp and lstm model showed overfitting. I address the issue by decrease learning rate, added drop out to the model structure, reduce number of epochs and add weight decay. \n",
    "Using class weight makes the model to focus more on minority classes, it solves the problem of imbalanced proportion of negative to neutral to positive samples, increase the macro F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960ad26",
   "metadata": {},
   "source": [
    "2. Model Performance and Error Analysis\n",
    "\n",
    "- Which of your two models generalized better to the test set? Provide evidence from your metrics.\n",
    "- Which sentiment class was most frequently misclassified? Propose reasons for this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ac811",
   "metadata": {},
   "source": [
    "Lstm model generalized better to the test set. It has higher accuracy and f1 macro score with slightly higher loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00a83d",
   "metadata": {},
   "source": [
    "Test Metrics from lstm model: \n",
    "![Test Metrics](outputs/lstm_test_metric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab920e",
   "metadata": {},
   "source": [
    "Test Metrics from mlp model: \n",
    "![Test Metrics](outputs/mlp_test_metric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054098a",
   "metadata": {},
   "source": [
    "Positive class are most frequently misclassified, because in financial phrase bank dataset, the boundary between positive and neurtal are usually vague and hard to distiguish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c6fd5",
   "metadata": {},
   "source": [
    "Test Confusion Matrix from lstm model: \n",
    "![Confusion Matrix](outputs/lstm_test_confusion_matrix_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea43a2",
   "metadata": {},
   "source": [
    "Test confusion matrix from MLP model:\n",
    "![Confusion Matrix](outputs/mlp_test_confusion_matrix_normalized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03798953",
   "metadata": {},
   "source": [
    "3. Cross-Model Comparison\n",
    "Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT\n",
    "\n",
    "- How did mean-pooled FastText embeddings limit the MLP compared to sequence-based models?\n",
    "- What advantage did the LSTMâ€™s sequential processing provide over the MLP?\n",
    "- Did fine-tuned LLMs (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations.\n",
    "- Rank all six models by test performance. What architectural or representational factors explain the ranking?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de514c5f",
   "metadata": {},
   "source": [
    "MLP:\n",
    "Test F1 Macro: 0.6806"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db1ccc",
   "metadata": {},
   "source": [
    "rnn: \n",
    "Final Test Accuracy: 0.7166\n",
    "Test F1 Macro: 0.6878\n",
    "Test F1 Weighted: 0.7228"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99e063",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "GRU:\n",
    "Final Test Accuracy: 0.7235\n",
    "Test F1 Macro: 0.6937\n",
    "Test F1 Weighted: 0.7301"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f3197",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "LSTM:\n",
    "Test F1 Macro: 0.7811"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f19f1c5",
   "metadata": {},
   "source": [
    "BERT:\n",
    "\n",
    "Final Test Accuracy: 0.8253\n",
    "Test F1 Macro: 0.8124\n",
    "Test F1 Weighted: 0.8270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbe236",
   "metadata": {},
   "source": [
    "GPT:\n",
    "\n",
    "Final Test Accuracy: 0.8267\n",
    "Test F1 Macro: 0.7936\n",
    "Test F1 Weighted: 0.8255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc955504",
   "metadata": {},
   "source": [
    "mean-pooling eliminate the order of words in a sentence which limits the performance of mlp model.\n",
    "LSTM model captures the order within a sentence which is an advantage over MLP model.\n",
    "Yes, Bert, GPT outperform classical baselines because they are pretrained on larger and more comprehensive dataset, and they also use better pretrained embeddings. These allows the models to better capture sentence meaning. \n",
    "The trend is clear, from MLP to sequential model to attention based architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "I used ChatGPT to debug my code, provide advice on mlp sturcture, and provide guildance on fine tuning hypterparameters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat359-su25-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
